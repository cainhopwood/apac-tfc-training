{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Day 1: K Nearest Neighbors Model for the Product Safety Dataset\n",
    "\n",
    "For the final project, build a K Nearest Neighbors model to predict the __human_tag__ field of the dataset. You will submit your predictions to the Leaderboard competition here: https://leaderboard.corp.amazon.com/tasks/352\n",
    "\n",
    "Use the notebooks from the class and implement the model, train and test with the corresponding datasets. Differently from the in-class exercise (regressor), you will develop a __classifier__. We are using F1 score to rank submissions. Sklearn provides the [__f1_score():__](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) function if you want to see how your model works on your training or validation set.\n",
    "\n",
    "You can follow these steps:\n",
    "1. Read training-test data (Given)\n",
    "2. Train a KNN classifier (Implement)\n",
    "3. Make predictions on your test dataset (Implement)\n",
    "4. Write your test predictions to a CSV file (Given)\n",
    "\n",
    "__You can use the KNN Classifier from here: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading the dataset\n",
    "\n",
    "We will use the __pandas__ library to read our dataset. Let's first run the following credential cell and then download the files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Training data:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>title</th>\n",
       "      <th>human_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47490</td>\n",
       "      <td>15808037321</td>\n",
       "      <td>I ordered a sample of the Dietspotlight Burn, ...</td>\n",
       "      <td>6/25/2018 17:51</td>\n",
       "      <td>1</td>\n",
       "      <td>DO NOT BUY!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16127</td>\n",
       "      <td>16042300811</td>\n",
       "      <td>This coffee tasts terrible as if it got burnt ...</td>\n",
       "      <td>2/8/2018 15:59</td>\n",
       "      <td>2</td>\n",
       "      <td>Coffee not good</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51499</td>\n",
       "      <td>16246716471</td>\n",
       "      <td>I've been buying lightly salted Planters cashe...</td>\n",
       "      <td>3/22/2018 17:53</td>\n",
       "      <td>2</td>\n",
       "      <td>Poor Quality - Burnt, Shriveled Nuts With Blac...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36725</td>\n",
       "      <td>14460351031</td>\n",
       "      <td>This product is great in so many ways. It goes...</td>\n",
       "      <td>12/7/2017 8:49</td>\n",
       "      <td>4</td>\n",
       "      <td>Very lovey product, good sunscreen, but strong...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49041</td>\n",
       "      <td>15509997211</td>\n",
       "      <td>My skin did not agree with this product, it wo...</td>\n",
       "      <td>3/21/2018 13:51</td>\n",
       "      <td>1</td>\n",
       "      <td>Not for everyone. Reactions can be harsh.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID       doc_id                                               text  \\\n",
       "0  47490  15808037321  I ordered a sample of the Dietspotlight Burn, ...   \n",
       "1  16127  16042300811  This coffee tasts terrible as if it got burnt ...   \n",
       "2  51499  16246716471  I've been buying lightly salted Planters cashe...   \n",
       "3  36725  14460351031  This product is great in so many ways. It goes...   \n",
       "4  49041  15509997211  My skin did not agree with this product, it wo...   \n",
       "\n",
       "              date  star_rating  \\\n",
       "0  6/25/2018 17:51            1   \n",
       "1   2/8/2018 15:59            2   \n",
       "2  3/22/2018 17:53            2   \n",
       "3   12/7/2017 8:49            4   \n",
       "4  3/21/2018 13:51            1   \n",
       "\n",
       "                                               title  human_tag  \n",
       "0                                        DO NOT BUY!          0  \n",
       "1                                    Coffee not good          0  \n",
       "2  Poor Quality - Burnt, Shriveled Nuts With Blac...          0  \n",
       "3  Very lovey product, good sunscreen, but strong...          0  \n",
       "4          Not for everyone. Reactions can be harsh.          1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('data/final_project/training.csv', encoding='utf-8', header=0)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Test data:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62199</td>\n",
       "      <td>15449606311</td>\n",
       "      <td>Quality of material is great, however, the bac...</td>\n",
       "      <td>3/7/2018 19:47</td>\n",
       "      <td>3</td>\n",
       "      <td>great backpack with strange fit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76123</td>\n",
       "      <td>15307152511</td>\n",
       "      <td>The product was okay but wasn't refined campho...</td>\n",
       "      <td>43135.875</td>\n",
       "      <td>2</td>\n",
       "      <td>Not refined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78742</td>\n",
       "      <td>12762748321</td>\n",
       "      <td>I normally read the reviews before buying some...</td>\n",
       "      <td>42997.37708</td>\n",
       "      <td>1</td>\n",
       "      <td>Doesnt work, wouldnt recommend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64010</td>\n",
       "      <td>15936405041</td>\n",
       "      <td>These pads are completely worthless. The light...</td>\n",
       "      <td>43313.25417</td>\n",
       "      <td>1</td>\n",
       "      <td>The lighter colored side of the pads smells li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17058</td>\n",
       "      <td>13596875291</td>\n",
       "      <td>The saw works great but the blade oiler does n...</td>\n",
       "      <td>12/5/2017 20:17</td>\n",
       "      <td>2</td>\n",
       "      <td>The saw works great but the blade oiler does n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID       doc_id                                               text  \\\n",
       "0  62199  15449606311  Quality of material is great, however, the bac...   \n",
       "1  76123  15307152511  The product was okay but wasn't refined campho...   \n",
       "2  78742  12762748321  I normally read the reviews before buying some...   \n",
       "3  64010  15936405041  These pads are completely worthless. The light...   \n",
       "4  17058  13596875291  The saw works great but the blade oiler does n...   \n",
       "\n",
       "              date  star_rating  \\\n",
       "0   3/7/2018 19:47            3   \n",
       "1        43135.875            2   \n",
       "2      42997.37708            1   \n",
       "3      43313.25417            1   \n",
       "4  12/5/2017 20:17            2   \n",
       "\n",
       "                                               title  \n",
       "0                    great backpack with strange fit  \n",
       "1                                        Not refined  \n",
       "2                     Doesnt work, wouldnt recommend  \n",
       "3  The lighter colored side of the pads smells li...  \n",
       "4  The saw works great but the blade oiler does n...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_df = pd.read_csv('data/final_project/test.csv', encoding='utf-8', header=0)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train a KNN Classifier\n",
    "Here, you will apply pre-processing operations in the class. Then, you can split your dataset to training and validation here. For your first submission, you will use __K Nearest Neighbors Classifier__. It is available [here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html). In the competition, we are using the F1 score. In sklearn, you can use the [__f1_score():__](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) function to see your F1 score on your training or validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analysis and missing value imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the range and distribution of __human_tag__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_df[\"human_tag\"].plot.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the range and distribution of __star_rating__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_df[\"star_rating\"].plot.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the number of missing values for each columm below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID             0\n",
      "doc_id         0\n",
      "text           6\n",
      "date           0\n",
      "star_rating    0\n",
      "title          1\n",
      "human_tag      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fill-in the missing values for __reviewText__ below. We will just use the placeholder \"Missing\" here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"text\"].fillna(\"Missing\", inplace=True)\n",
    "train_df[\"title\"].fillna(\"Missing\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Stop word removal and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install the library and functions\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create the stop word removal and text cleaning processes below. NLTK library provides a list of common stop words. We will use the list, but remove some of the words from that list. It is because those words are actually useful to understand the sentiment in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Let's get a list of stop words from the NLTK library\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# These words are important for our problem. We don't want to remove them.\n",
    "excluding = ['against', 'not', 'don', \"don't\",'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\n",
    "             'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", \n",
    "             'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\",\n",
    "             'needn', \"needn't\",'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \n",
    "             \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "# New stop word list\n",
    "stop_words = [word for word in stop if word not in excluding]\n",
    "\n",
    "snow = SnowballStemmer('english')\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "# This is a helper function to map NTLK position tags\n",
    "# Full list is available here: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def process_text(texts, lem): \n",
    "    if (lem) :\n",
    "        print (\"Processing using lematisation\")\n",
    "    else:\n",
    "        print (\"Processing using stemming\")\n",
    "        \n",
    "    final_text_list=[]\n",
    "    for sent in texts:\n",
    "        filtered_sentence=[]\n",
    "        processed_sentence = []\n",
    "        \n",
    "        sent = sent.lower() # Lowercase \n",
    "        sent = sent.strip() # Remove leading/trailing whitespace\n",
    "        sent = re.sub('\\s+', ' ', sent) # Remove extra space and tabs\n",
    "        sent = re.compile('<.*?>').sub('', sent) # Remove HTML tags/markups:\n",
    "        \n",
    "        for w in word_tokenize(sent):\n",
    "            # We are applying some custom filtering here, feel free to try different things\n",
    "            # Check if it is not numeric and its length>2 and not in stop words\n",
    "            if(not w.isnumeric()) and (len(w)>2) and (w not in stop_words):  \n",
    "                # possibly Stem and add to filtered list\n",
    "                if (lem):\n",
    "                    filtered_sentence.append(w)\n",
    "                else :\n",
    "                    filtered_sentence.append(snow.stem(w))\n",
    "        if (lem) :\n",
    "            # Get position tags\n",
    "            word_pos_tags = nltk.pos_tag(filtered_sentence)\n",
    "            # Map the position tag and lemmatize the word/token\n",
    "            for idx, tag in enumerate(word_pos_tags):\n",
    "                processed_sentence.append(wl.lemmatize(tag[0], get_wordnet_pos(tag[1])))\n",
    "            final_string = \" \".join(processed_sentence) #lematised sentence\n",
    "        else :\n",
    "            final_string = \" \".join(filtered_sentence) #final string of cleaned words\n",
    " \n",
    "        final_text_list.append(final_string)\n",
    "    \n",
    "    return final_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing the text field\n",
      "Processing using lematisation\n",
      "Pre-processing the title field\n",
      "Processing using lematisation\n"
     ]
    }
   ],
   "source": [
    "# use lemmatisation gloabl var\n",
    "useLem = 1\n",
    "\n",
    "print(\"Pre-processing the text field\")\n",
    "train_df[\"text\"] = process_text(train_df[\"text\"].tolist(), useLem) \n",
    "print(\"Pre-processing the title field\")\n",
    "train_df[\"title\"] = process_text(train_df[\"title\"].tolist(), useLem) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Scaling numerical fields:\n",
    "\n",
    "We will apply min-max scaling to our rating field so that they will be between 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"star_rating\"] = (train_df[\"star_rating\"] - train_df[\"star_rating\"].min())/(train_df[\"star_rating\"].max()-train_df[\"star_rating\"].min())\n",
    "#train_df[\"time\"] = (train_df[\"time\"] - train_df[\"time\"].min())/(train_df[\"time\"].max()-train_df[\"time\"].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Splitting the training dataset into training and validation\n",
    "\n",
    "Sklearn library has a useful function to split datasets. We will use the __train_test_split()__ function. In the example below, we get 90% of the data for training and 10% is left for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Input: \"text\", \"star_rating\"\n",
    "# Target: \"uman_tag\"\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df[[\"text\", \"title\", \"star_rating\"]],\n",
    "                                                  train_df[\"human_tag\"].tolist(),\n",
    "                                                  test_size=0.10,\n",
    "                                                  shuffle=True\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Computing Bag of Words features\n",
    "\n",
    "We are using binary features here. TF and TF-IDF are also other options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Initialize the binary count vectorizer\n",
    "tfidf_vectorizer = CountVectorizer(binary=True,\n",
    "                                   max_features=200    # Limit the vocabulary size\n",
    "                                  )\n",
    "tfidf_vectorizer_title = CountVectorizer(binary=True,\n",
    "                                   max_features=100    # Limit the vocabulary size\n",
    "                                  )\n",
    "# Fit and transform\n",
    "X_train_text_vectors = tfidf_vectorizer.fit_transform(X_train[\"text\"].tolist())\n",
    "X_train_title_vectors = tfidf_vectorizer_title.fit_transform(X_train[\"title\"].tolist())\n",
    "# Only transform\n",
    "X_val_text_vectors = tfidf_vectorizer.transform(X_val[\"text\"].tolist())\n",
    "X_val_title_vectors = tfidf_vectorizer_title.transform(X_val[\"title\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print our vocabulary below. The number next to the word is its index in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'look': 94, 'good': 62, 'however': 77, 'start': 165, 'could': 31, 'see': 151, 'burnt': 21, 'never': 109, 'back': 10, 'make': 98, 'pay': 122, 'return': 145, 'bad': 11, 'item': 81, 'amazon': 5, 'get': 59, 'not': 113, 'two': 182, 'different': 37, 'problem': 130, 'expect': 47, 'fire': 55, 'light': 89, 'work': 196, 'box': 16, 'smell': 161, 'like': 90, 'something': 163, 'burn': 19, 'still': 168, 'come': 29, 'also': 3, 'would': 198, 'place': 125, 'close': 27, 're': 136, 'try': 180, 'stick': 167, 'even': 43, 'use': 184, 'size': 158, 'every': 45, 'actually': 1, 'pretty': 127, 'away': 9, 'easy': 40, 'people': 123, 'clean': 26, 'take': 171, 'remove': 143, 'heat': 70, 'hard': 69, 'keep': 82, 'break': 18, 'piece': 124, 'stay': 166, 'taste': 172, 'great': 63, 'day': 34, 'week': 192, 'help': 71, 'need': 108, 'money': 105, 'second': 150, 'time': 178, 'purchase': 132, 'product': 131, 'first': 56, 'really': 138, 'want': 186, 'think': 176, 'buy': 22, 'long': 93, 'around': 8, 'it': 80, 'hair': 64, 've': 185, 'anything': 7, 'much': 107, 'oil': 116, 'several': 155, 'month': 106, 'open': 119, 'leave': 86, 'old': 117, 'eye': 48, 'without': 195, 'waste': 188, 'well': 193, 'say': 149, 'put': 133, 'find': 52, 'one': 118, 'way': 190, 'another': 6, 'little': 91, 'since': 157, 'though': 177, 'big': 13, 'review': 146, 'may': 100, 'receive': 140, 'almost': 2, 'cook': 30, 'set': 154, 'turn': 181, 'last': 85, 'go': 61, 'notice': 115, 'food': 58, 'many': 99, 'unit': 183, 'love': 96, 'easily': 39, 'finger': 54, 'wear': 191, 'feel': 51, 'give': 60, 'star': 164, 'seem': 152, 'wish': 194, 'year': 199, 'replace': 144, 'far': 50, 'send': 153, 'll': 92, 'always': 4, 'run': 148, 'part': 121, 'reason': 139, 'half': 65, 'quality': 134, 'color': 28, 'cheap': 25, 'burning': 20, 'hand': 66, 'top': 179, 'order': 120, 'battery': 12, 'low': 97, 'thing': 175, 'high': 72, 'know': 83, 'new': 110, 'price': 128, 'probably': 129, 'recommend': 141, 'might': 103, 'cause': 23, 'end': 41, 'hot': 74, 'able': 0, 'handle': 67, 'ever': 44, 'minute': 104, 'hour': 75, 'enough': 42, 'cut': 33, 'plastic': 126, 'face': 49, 'wash': 187, 'lot': 95, 'skin': 159, 'small': 160, 'happen': 68, 'couple': 32, 'bottom': 15, 'everything': 46, 'sure': 170, 'read': 137, 'next': 111, 'house': 76, 'water': 189, 'inside': 78, 'less': 87, 'the': 174, 'large': 84, 'dry': 38, 'stop': 169, 'red': 142, 'bit': 14, 'issue': 79, 'let': 88, 'right': 147, 'worth': 197, 'maybe': 101, 'melt': 102, 'tell': 173, 'nothing': 114, 'fit': 57, 'brand': 17, 'side': 156, 'hold': 73, 'charge': 24, 'smoke': 162, 'design': 36, 'nice': 112, 'fine': 53, 'quickly': 135, 'definitely': 35}\n",
      "{'buy': 7, 'review': 72, 'waste': 92, 'money': 55, 'fire': 28, 'try': 88, 'like': 49, 'not': 60, 'easily': 19, 'bad': 0, 'well': 95, 'look': 52, 'much': 57, 'work': 96, 'great': 35, 'light': 48, 'year': 99, 'good': 34, 'easy': 20, 'clean': 11, 'nice': 59, 'hot': 42, 'item': 43, 'hand': 37, 'one': 61, 'star': 79, 'cheap': 10, 'dangerous': 15, 'hair': 36, 'burn': 4, 'battery': 1, 'job': 44, 'price': 66, 'terrible': 84, 'quality': 69, 'use': 90, 'taste': 83, 'junk': 45, 'still': 81, 'plastic': 63, 'handle': 38, 'heat': 40, 'careful': 8, 'love': 53, 'burnt': 6, 'want': 91, 'really': 70, 'feel': 26, 'product': 67, 'skin': 76, 'start': 80, 'ever': 22, 'two': 89, 'poor': 64, 'fine': 27, 'month': 56, 'eye': 24, 'best': 2, 'four': 31, 'make': 54, 'smell': 78, 'small': 77, 'happy': 39, 'would': 98, 'recommend': 71, 'disappointed': 18, 'three': 86, 'worth': 97, 'seem': 73, 'burning': 5, 'day': 16, 'time': 87, 'first': 29, 'beware': 3, 'give': 33, 'size': 75, 'get': 32, 'thing': 85, 'long': 51, 'color': 12, 'design': 17, 'pretty': 65, 'need': 58, 'expect': 23, 'little': 50, 'come': 13, 'horrible': 41, 'way': 93, 'perfect': 62, 'even': 21, 'sensitive': 74, 'last': 47, 'keep': 46, 'purchase': 68, 'fit': 30, 'face': 25, 'cause': 9, 'take': 82, 'could': 14, 'week': 94}\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vectorizer.vocabulary_)\n",
    "print(tfidf_vectorizer_title.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Fitting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from time import time\n",
    "start = time()\n",
    "\n",
    "# Let' merge our features\n",
    "X_train_features = np.column_stack((X_train_text_vectors.toarray(), \n",
    "                                    X_train_title_vectors.toarray(),\n",
    "                                    X_train[\"star_rating\"].values )\n",
    "                                  )\n",
    "\n",
    "# Using the default KNN with 5 nearest neighbors\n",
    "knnClass = KNeighborsClassifier(n_neighbors=5)\n",
    "knnClass.fit(X_train_features, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Checking model performance on the validation dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Training and validation time for one value of K (in seconds): 362.6898639202118\n",
      "Mean_squared_error: 0.171207,  f1_score: 0.269101\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, mean_squared_error\n",
    "\n",
    "X_val_features = np.column_stack((X_val_text_vectors.toarray(), \n",
    "                                  X_val_title_vectors.toarray(),\n",
    "                                  X_val[\"star_rating\"].values) )\n",
    "\n",
    "val_predictions = knnClass.predict(X_val_features)\n",
    "\n",
    "end = time()\n",
    "print('KNN Training and validation time for one value of K (in seconds):', end-start)\n",
    "\n",
    "print(\"Mean_squared_error: %f,  f1_score: %f\" % (mean_squared_error(y_val, val_predictions), f1_score(y_val, val_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Trying different K values\n",
    "\n",
    "Let's try different K values and see how the model performs with each one.\n",
    "\n",
    "*Note: When experimenting with different values of K, keep in mind that KNN training and validation for one value of K can take around 1 minute*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K_values = [4, 5, 6, 7]\n",
    "\n",
    "for K in K_values:\n",
    "    knnClass = KNeighborsClassifier(n_neighbors=K)\n",
    "    knnClass.fit(X_train_features, y_train)\n",
    "    val_predictions = knnClass.predict(X_val_features)\n",
    "    print(\"K=%d, Mean_squared_error: %f,  f1_score: %f\" % (K, mean_squared_error(y_val, val_predictions), f1_score(y_val, val_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Make predictions on your test dataset\n",
    "\n",
    "Once we select our best performing model, we can use it to make predictions on the test dataset. You can simply use __.fit()__ function with your training data to use the best performing K value and use __.predict()__ with your test data to get your test predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing the text field\n",
      "Processing using lematisation\n",
      "Pre-processing the title field\n",
      "Processing using lematisation\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Pre process the test dataset same as we did training\n",
    "test_df[\"text\"].fillna(\"Missing\", inplace=True)\n",
    "test_df[\"title\"].fillna(\"Missing\", inplace=True)\n",
    "print(\"Pre-processing the text field\")\n",
    "test_df[\"text\"] = process_text(test_df[\"text\"].tolist(), useLem) \n",
    "print(\"Pre-processing the title field\")\n",
    "test_df[\"title\"] = process_text(test_df[\"title\"].tolist(), useLem) \n",
    "test_text_vectors = tfidf_vectorizer.transform(test_df[\"text\"].tolist())\n",
    "test_title_vectors = tfidf_vectorizer_title.transform(test_df[\"title\"].tolist())\n",
    "#numerics\n",
    "test_df[\"star_rating\"] = (test_df[\"star_rating\"] - test_df[\"star_rating\"].min())/(test_df[\"star_rating\"].max()-test_df[\"star_rating\"].min())\n",
    "\n",
    "# Using the default KNN with 5 nearest neighbors\n",
    "knnClass = KNeighborsClassifier(n_neighbors=5)\n",
    "knnClass.fit(X_train_features, y_train)\n",
    "\n",
    "# Now predict the test dataset\n",
    "test_features = np.column_stack((test_text_vectors.toarray(), \n",
    "                                 test_title_vectors.toarray(),\n",
    "                                  test_df[\"star_rating\"].values) )\n",
    "\n",
    "test_predictions = knnClass.predict(test_features)\n",
    "\n",
    "print(test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Write your predictions to a CSV file\n",
    "You can use the following code to write your test predictions to a CSV file. Then upload your file to https://leaderboard.corp.amazon.com/tasks/352/submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "result_df = pd.DataFrame()\n",
    "result_df[\"ID\"] = test_df[\"ID\"]\n",
    "result_df[\"human_tag\"] = test_predictions\n",
    " \n",
    "result_df.to_csv(\"project_day1_result_2.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
